{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets  \n",
    "### (Predict which Tweets are about real disasters and which ones are not)\n",
    "\n",
    "[Link to problem statement](https://www.kaggle.com/competitions/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../nlp-getting-started/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(\"NA\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def remove_link(sentence):\n",
    "    cleaned_sentence = re.sub(r'http\\S+|www\\S+', '', sentence)\n",
    "    return cleaned_sentence.strip()\n",
    "\n",
    "def extractHashtags(text):\n",
    "    li = re.findall(r'\\#([a-zA-Z0-9_]+)', text)\n",
    "    return li\n",
    "\n",
    "def remove_words_starting_with_at(sentence):\n",
    "    cleaned_sentence = re.sub(r'\\@\\w+\\s*', '', sentence)\n",
    "    return cleaned_sentence.strip()\n",
    "\n",
    "def removePunctuations(text):\n",
    "    newText = \"\".join([i for i in text if i not in punctuation])\n",
    "    return newText\n",
    "\n",
    "def removeStopwords(text):\n",
    "    newtext = [i for i in text.split() if i not in stopwords.words(\"english\")]\n",
    "    return newtext\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    newText = [wnl.lemmatize(ele) for ele in text]\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_nolink'] = data['text'].apply(lambda x:remove_link(x))\n",
    "data['hashtags'] = data['text_nolink'].apply(lambda x:extractHashtags(x))\n",
    "data['text_nomentions'] = data['text_nolink'].apply(lambda x:remove_words_starting_with_at(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def findLocations(text):\n",
    "    doc = nlp(text)\n",
    "    locations = [entity.text for entity in doc.ents if entity.label_ == 'GPE' or entity.label_ == 'LOC']\n",
    "    return locations\n",
    "\n",
    "data['extracted_locations'] = data['text_nomentions'].apply(lambda x:findLocations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['noPunctuations'] = data['text_nomentions'].apply(lambda x:removePunctuations(x))\n",
    "data['noStopwordsTokenized'] = data['noPunctuations'].apply(lambda x:removeStopwords(x))\n",
    "data['lemmatized'] = data['noStopwordsTokenized'].apply(lambda x:lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the each string in the array with a space in between\n",
    "def finalize(textList):\n",
    "    text = \" \".join(textList)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_nolink</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_nomentions</th>\n",
       "      <th>extracted_locations</th>\n",
       "      <th>noPunctuations</th>\n",
       "      <th>noStopwordsTokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[Our, Deeds, Reason, earthquake, May, ALLAH, F...</td>\n",
       "      <td>[Our, Deeds, Reason, earthquake, May, ALLAH, F...</td>\n",
       "      <td>our deeds reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[]</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[Canada]</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>[All, residents, asked, shelter, place, notifi...</td>\n",
       "      <td>[All, resident, asked, shelter, place, notifie...</td>\n",
       "      <td>all resident asked shelter place notified offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[wildfires]</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[California]</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[Ruby, Alaska]</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, Alaska, smoke, ...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, Alaska, smoke, ...</td>\n",
       "      <td>just got sent photo ruby alaska smoke wildfire...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1      NA       NA  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4      NA       NA             Forest fire near La Ronge Sask. Canada   \n",
       "2   5      NA       NA  All residents asked to 'shelter in place' are ...   \n",
       "3   6      NA       NA  13,000 people receive #wildfires evacuation or...   \n",
       "4   7      NA       NA  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                        text_nolink  \\\n",
       "0       1  Our Deeds are the Reason of this #earthquake M...   \n",
       "1       1             Forest fire near La Ronge Sask. Canada   \n",
       "2       1  All residents asked to 'shelter in place' are ...   \n",
       "3       1  13,000 people receive #wildfires evacuation or...   \n",
       "4       1  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "              hashtags                                    text_nomentions  \\\n",
       "0         [earthquake]  Our Deeds are the Reason of this #earthquake M...   \n",
       "1                   []             Forest fire near La Ronge Sask. Canada   \n",
       "2                   []  All residents asked to 'shelter in place' are ...   \n",
       "3          [wildfires]  13,000 people receive #wildfires evacuation or...   \n",
       "4  [Alaska, wildfires]  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "  extracted_locations                                     noPunctuations  \\\n",
       "0                  []  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1            [Canada]              Forest fire near La Ronge Sask Canada   \n",
       "2                  []  All residents asked to shelter in place are be...   \n",
       "3        [California]  13000 people receive wildfires evacuation orde...   \n",
       "4      [Ruby, Alaska]  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                noStopwordsTokenized  \\\n",
       "0  [Our, Deeds, Reason, earthquake, May, ALLAH, F...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2  [All, residents, asked, shelter, place, notifi...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [Just, got, sent, photo, Ruby, Alaska, smoke, ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [Our, Deeds, Reason, earthquake, May, ALLAH, F...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2  [All, resident, asked, shelter, place, notifie...   \n",
       "3  [13000, people, receive, wildfire, evacuation,...   \n",
       "4  [Just, got, sent, photo, Ruby, Alaska, smoke, ...   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0    our deeds reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  all resident asked shelter place notified offi...  \n",
       "3  13000 people receive wildfire evacuation order...  \n",
       "4  just got sent photo ruby alaska smoke wildfire...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_cleaned'] = data['lemmatized'].apply(lambda x:finalize(x).lower())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['location'] = data['location'].apply(lambda x:remove_link(x))\n",
    "data['location'] = data['location'].apply(lambda x:remove_words_starting_with_at(x))\n",
    "data['location'] = data['location'].apply(lambda x:removePunctuations(x))\n",
    "data['location'] = data['location'].apply(lambda x:removeStopwords(x))\n",
    "data['location'] = data['location'].apply(lambda x:lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'] = data['keyword'].apply(lambda x:remove_link(x))\n",
    "data['keyword'] = data['keyword'].apply(lambda x:remove_words_starting_with_at(x))\n",
    "data['keyword'] = data['keyword'].apply(lambda x:removePunctuations(x))\n",
    "data['keyword'] = data['keyword'].apply(lambda x:removeStopwords(x))\n",
    "data['keyword'] = data['keyword'].apply(lambda x:lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['location'] = data['location'].apply(lambda x:finalize(x))\n",
    "data['keyword'] = data['keyword'].apply(lambda x:finalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['all_locations'] = data['location'] + ' ' + data['extracted_locations'].apply(lambda x: ' '.join(x))\n",
    "data['all_locations'] = data['all_locations'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNA(li):\n",
    "    ans = []\n",
    "    for el in li:\n",
    "        if el != 'NA':\n",
    "            ans.append(el)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['all_locations'] = data['all_locations'].apply(lambda x:removeNA(x))\n",
    "data['all_locations'] = data['all_locations'].apply(lambda x:list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_elements(row):\n",
    "    return [item for item in row['hashtags'] if item not in row['all_locations']]\n",
    "\n",
    "data['extra_keywords'] = data.apply(extract_unique_elements, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['all_keywords'] = data['keyword'] + ' ' + data['extra_keywords'].apply(lambda x:' '.join(x))\n",
    "data['all_keywords'] = data['all_keywords'].apply(lambda x:x.split())\n",
    "data['all_keywords'] = data['all_keywords'].apply(lambda x:removeNA(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['all_locations'] = data['all_locations'].apply(lambda x:finalize(x))\n",
    "data['all_keywords'] = data['all_keywords'].apply(lambda x:finalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_nolink</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text_nomentions</th>\n",
       "      <th>extracted_locations</th>\n",
       "      <th>noPunctuations</th>\n",
       "      <th>noStopwordsTokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>all_locations</th>\n",
       "      <th>extra_keywords</th>\n",
       "      <th>all_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[Our, Deeds, Reason, earthquake, May, ALLAH, F...</td>\n",
       "      <td>[Our, Deeds, Reason, earthquake, May, ALLAH, F...</td>\n",
       "      <td>our deeds reason earthquake may allah forgive u</td>\n",
       "      <td></td>\n",
       "      <td>[earthquake]</td>\n",
       "      <td>earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[]</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[Canada]</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>Canada</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>[All, residents, asked, shelter, place, notifi...</td>\n",
       "      <td>[All, resident, asked, shelter, place, notifie...</td>\n",
       "      <td>all resident asked shelter place notified offi...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[wildfires]</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[California]</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "      <td>California</td>\n",
       "      <td>[wildfires]</td>\n",
       "      <td>wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[Alaska, wildfires]</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[Ruby, Alaska]</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, Alaska, smoke, ...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, Alaska, smoke, ...</td>\n",
       "      <td>just got sent photo ruby alaska smoke wildfire...</td>\n",
       "      <td>Ruby Alaska</td>\n",
       "      <td>[wildfires]</td>\n",
       "      <td>wildfires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1      NA       NA  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4      NA       NA             Forest fire near La Ronge Sask. Canada   \n",
       "2   5      NA       NA  All residents asked to 'shelter in place' are ...   \n",
       "3   6      NA       NA  13,000 people receive #wildfires evacuation or...   \n",
       "4   7      NA       NA  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                        text_nolink  \\\n",
       "0       1  Our Deeds are the Reason of this #earthquake M...   \n",
       "1       1             Forest fire near La Ronge Sask. Canada   \n",
       "2       1  All residents asked to 'shelter in place' are ...   \n",
       "3       1  13,000 people receive #wildfires evacuation or...   \n",
       "4       1  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "              hashtags                                    text_nomentions  \\\n",
       "0         [earthquake]  Our Deeds are the Reason of this #earthquake M...   \n",
       "1                   []             Forest fire near La Ronge Sask. Canada   \n",
       "2                   []  All residents asked to 'shelter in place' are ...   \n",
       "3          [wildfires]  13,000 people receive #wildfires evacuation or...   \n",
       "4  [Alaska, wildfires]  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "  extracted_locations                                     noPunctuations  \\\n",
       "0                  []  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1            [Canada]              Forest fire near La Ronge Sask Canada   \n",
       "2                  []  All residents asked to shelter in place are be...   \n",
       "3        [California]  13000 people receive wildfires evacuation orde...   \n",
       "4      [Ruby, Alaska]  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                noStopwordsTokenized  \\\n",
       "0  [Our, Deeds, Reason, earthquake, May, ALLAH, F...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2  [All, residents, asked, shelter, place, notifi...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [Just, got, sent, photo, Ruby, Alaska, smoke, ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [Our, Deeds, Reason, earthquake, May, ALLAH, F...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2  [All, resident, asked, shelter, place, notifie...   \n",
       "3  [13000, people, receive, wildfire, evacuation,...   \n",
       "4  [Just, got, sent, photo, Ruby, Alaska, smoke, ...   \n",
       "\n",
       "                                        text_cleaned all_locations  \\\n",
       "0    our deeds reason earthquake may allah forgive u                 \n",
       "1              forest fire near la ronge sask canada        Canada   \n",
       "2  all resident asked shelter place notified offi...                 \n",
       "3  13000 people receive wildfire evacuation order...    California   \n",
       "4  just got sent photo ruby alaska smoke wildfire...   Ruby Alaska   \n",
       "\n",
       "  extra_keywords all_keywords  \n",
       "0   [earthquake]   earthquake  \n",
       "1             []               \n",
       "2             []               \n",
       "3    [wildfires]    wildfires  \n",
       "4    [wildfires]    wildfires  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_cleaned[['text_cleaned', 'all_locations', 'all_keywords', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_20720\\3580836748.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_length'] = df['text_cleaned'].apply(lambda x:len(x))\n",
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_20720\\3580836748.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['word_count'] = df['text_cleaned'].apply(lambda x:len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "df['text_length'] = df['text_cleaned'].apply(lambda x:len(x))\n",
    "df['word_count'] = df['text_cleaned'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_cleaned     0\n",
       "all_locations    0\n",
       "all_keywords     0\n",
       "target           0\n",
       "text_length      0\n",
       "word_count       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_20720\\746741227.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(\"NA\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.fillna(\"NA\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_cleaned     0\n",
       "all_locations    0\n",
       "all_keywords     0\n",
       "target           0\n",
       "text_length      0\n",
       "word_count       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "text_vectorized = cv.fit_transform(df['text_cleaned']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(text_vectorized, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0011</th>\n",
       "      <th>001116</th>\n",
       "      <th>0025</th>\n",
       "      <th>005225</th>\n",
       "      <th>010156</th>\n",
       "      <th>010217</th>\n",
       "      <th>0104</th>\n",
       "      <th>010401</th>\n",
       "      <th>0106</th>\n",
       "      <th>0111</th>\n",
       "      <th>...</th>\n",
       "      <th>ûòåêcnbc</th>\n",
       "      <th>ûó</th>\n",
       "      <th>ûóbbc</th>\n",
       "      <th>ûóher</th>\n",
       "      <th>ûókody</th>\n",
       "      <th>ûónegligence</th>\n",
       "      <th>ûótech</th>\n",
       "      <th>ûówe</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14996 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0011  001116  0025  005225  010156  010217  0104  010401  0106  0111  ...  \\\n",
       "0     0       0     0       0       0       0     0       0     0     0  ...   \n",
       "1     0       0     0       0       0       0     0       0     0     0  ...   \n",
       "2     0       0     0       0       0       0     0       0     0     0  ...   \n",
       "3     0       0     0       0       0       0     0       0     0     0  ...   \n",
       "4     0       0     0       0       0       0     0       0     0     0  ...   \n",
       "\n",
       "   ûòåêcnbc  ûó  ûóbbc  ûóher  ûókody  ûónegligence  ûótech  ûówe  \\\n",
       "0         0   0      0      0       0             0       0     0   \n",
       "1         0   0      0      0       0             0       0     0   \n",
       "2         0   0      0      0       0             0       0     0   \n",
       "3         0   0      0      0       0             0       0     0   \n",
       "4         0   0      0      0       0             0       0     0   \n",
       "\n",
       "   text_length  word_count  \n",
       "0           47           8  \n",
       "1           37           7  \n",
       "2           92          13  \n",
       "3           57           7  \n",
       "4           59          10  \n",
       "\n",
       "[5 rows x 14996 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for el in ['text_length', 'word_count', 'target']:\n",
    "    data[el] = df[el]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Columns: 14996 entries, 0011 to word_count\n",
      "dtypes: int64(14996)\n",
      "memory usage: 871.0 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(['target'], axis=1)\n",
    "Y = data['target']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[630 688]\n",
      " [177 789]]\n",
      "0.6212784588441331\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "classifier1 = GaussianNB()\n",
    "classifier1.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = classifier1.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1049  269]\n",
      " [ 342  624]]\n",
      "0.7324868651488616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "treeClassifier1 = DecisionTreeClassifier()\n",
    "treeClassifier1.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = treeClassifier1.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1198  120]\n",
      " [ 368  598]]\n",
      "0.7863397548161121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1134  184]\n",
      " [ 272  694]]\n",
      "0.8003502626970228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=1, max_iter=7600, random_state=42)\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = lr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1204  114]\n",
      " [ 337  629]]\n",
      "0.8025394045534151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = bnb.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(accuracy_score(Y_test, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
